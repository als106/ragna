from fastapi import FastAPI
from pydantic import BaseModel
from vector import retriever
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableSequence
from dotenv import load_dotenv
import os
import logging

# Configuraci√≥n de logs
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Cargar variables de entorno
load_dotenv()

# -------- CONFIGURACI√ìN --------
MODEL_NAME = "gemma2:2b"
app = FastAPI()


class Query(BaseModel):
    question: str
    session_id: str = "default"


model = OllamaLLM(model=MODEL_NAME)

prompt = ChatPromptTemplate.from_template("""
Eres un asistente acad√©mico especializado en la Universidad de Alicante. Tu tarea es responder preguntas con informaci√≥n relevante y fiable extra√≠da exclusivamente del contexto proporcionado por los documentos de la UA.

‚ö†Ô∏è Muy importante:
- Si no encuentras la informaci√≥n en el contexto, responde: "No dispongo de informaci√≥n suficiente para responder a esta pregunta". En ese caso, **no a√±adas una lista de fuentes utilizadas**.
- No inventes leyes, reglamentos, fechas ni procedimientos.
- No completes con datos gen√©ricos si no est√°n presentes en el contexto.

üéì √Åmbitos que puedes cubrir: titulaciones, matr√≠cula, normativa de permanencia, calendario acad√©mico, becas, pr√°cticas, movilidad, etc.

Pregunta del usuario:
{question}

üìö Contexto recuperado:
{context}

‚úçÔ∏è Redacta una respuesta clara, breve y bien estructurada, con un tono profesional y accesible.
""")

chat_chain = RunnableSequence(prompt | model | StrOutputParser())

# Cargar token desde variable de entorno
cohere_token = os.getenv("COHERE_API_KEY")
if cohere_token is None:
    raise ValueError("‚ùå No se ha definido la variable de entorno COHERE_API_KEY")

compressor = CohereRerank(
    model="rerank-multilingual-v3.0", top_n=5, cohere_api_key=cohere_token
)

reranked_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)


@app.post("/chat")
def chat_endpoint(query: Query):
    """Endpoint de generaci√≥n de respuesta basada en contexto RAG."""
    try:
        context = "\n\n".join(
            [doc.page_content for doc in reranked_retriever.invoke(query.question)]
        )
        response = chat_chain.invoke({"context": context, "question": query.question})
        return {"response": response}
    except Exception as e:
        logging.error(f"‚ùå Error en API /chat: {e}")
        return {"response": "‚ö†Ô∏è Error al procesar la consulta."}
